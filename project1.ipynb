{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODS202 - Econometrics\n",
    "## Final Project - Nov 2024\n",
    "DE CARVALHO MACHADO PINHEIRO Rafaela  \n",
    "SANGINETO JUCA Marina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t, f\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Cross-section Data\n",
    "Using the HPRICE2.RAW dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HPRICE2.raw', delim_whitespace=True, header=None, names = [\"price\",\"crime\",\"nox\",\"rooms\",\"dist\",\"radial\",\"proptax\",\"stratio\",\"lowstat\",\"lprice\", \"lnox\", \"lproptax\"]) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. State the fundamental hypothesis under which the Ordinary Least Squares (OLS) estimators are unbiased.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental hypothesis under which the Ordinary Least Squares (OLS) estimators are unbiased is the Gauss-Markov Theorem. The Gauss-Markov Theorem states that the OLS estimators are unbiased and have the minimum variance among all linear unbiased estimators if the following assumptions are satisfied:  \n",
    "\n",
    "\n",
    "Linearity: The model is linear in the parameters.  \n",
    "Exogeneity: The expected value of the error term, given the explanatory variables, is zero.  \n",
    "Homoscedasticity: The variance of the error term is constant (does not depend on the explanatory variables).  \n",
    "No multicollinearity: The explanatory variables are not perfectly collinear.  \n",
    "No autocorrelation: The error terms are uncorrelated with each other.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental hypothesis under which the Ordinary Least Squares (OLS) estimators are unbiased is that the unobserved variable has a zero mean. So, in other words, to obtain unbiased estimators in a population model such as:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + u\n",
    "$$\n",
    "\n",
    "where $u$ is the error term, also known as the disturbance or unobserved variable, we need to ensure that the expected value of the error term conditioned on the independent variables(s) (X) is equal to the unconditional expected value, which is zero. Mathematically, this can be expressed as: \n",
    "\n",
    "$$\n",
    "E(u|X) = E(u) = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Show that under this assumption the OLS estimators are indeed unbiased.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that the OLS estimators are unbiased under the Gauss-Markov assumptions, we can use the properties of expected values:\n",
    "\n",
    "$$E[\\hat{\\beta}​]=E[(X′X)−1X′y]=E[(X′X)−1X′(X\\beta+ϵ)]=\\beta$$\n",
    "\n",
    "where β^​ is the OLS estimator, X is the matrix of explanatory variables, y is the dependent variable, and ϵ is the error term. The second equality follows from the linearity assumption, and the third equality follows from the exogeneity assumption (i.e., $E[ϵ∣X]=0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the following linear model:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_{K} x_{iK} + u_i\n",
    "$$\n",
    "\n",
    "in the matrix form, such as:\n",
    "\n",
    "$$\n",
    "y = X \\beta + u\n",
    "$$\n",
    "\n",
    "with $y = (y_1, ..., y_n)'$, $x_k = (x_{1k}, ..., x_{nk})'$, $u=(u_1,...,u_n)'$, $\\beta = [\\beta_1, ..., \\beta_K]'$ and $X = [x_1, ..., x_K]$. So, to derive OLS we need to find $\\beta$ that minimizes the following expression:\n",
    "\n",
    "$$\n",
    "u' u = (y - X \\beta)' (y - X \\beta)\n",
    "$$\n",
    "\n",
    "In order to do that, we use the fact that the orthogonality condition must be satisfied between X and u, leadind to:\n",
    "\n",
    "$$\n",
    "-2 X' (y - X \\beta) = 0\n",
    "$$\n",
    "\n",
    "Re-arranging considering that $(X'X)$ is inverted, since there is no multi-collinearity:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X' X)^{-1} X' y\n",
    "$$\n",
    "\n",
    "To be unbiased, the estimator above must satisfies:\n",
    "\n",
    "$$\n",
    "b(\\beta, \\hat{\\beta}) = E(\\hat{\\beta}) - \\beta = 0\n",
    "$$\n",
    "\n",
    "The estimator expectation is given by:\n",
    "\n",
    "$$\n",
    "E(\\hat{\\beta}) = E[(X'X)^{-1} X' (X \\beta + u)] = \\beta + E(X'u)\n",
    "$$\n",
    "\n",
    "So, in order to satisfy $b(\\beta, \\hat{\\beta}) = 0$, the fundamental hypothesis stated in the last item ($E(X'u) = 0$) must be satisfied.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Explain the sample selection bias with an example from the course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample selection bias occurs when the sample used for estimation is not representative of the population of interest. This can happen, for example, when the data is collected only from individuals who choose to participate in a study or program. An example from the course could be estimating the effect of a job training program on wages, but the data is only available for individuals who chose to participate in the program. In this case, the sample may not be representative of the entire population of eligible individuals, and the estimated effect may be biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the course, sample selection bias occurs when the sample used in analysis is not representative of the population under study.\n",
    "\n",
    "For instance, during World War II, a sample of RAF planes returning from war zones was used to determine areas that needed reinforcement. However, this sample excluded planes that were shot down, resulting in a biased sample. Consequently, the conclusion that reinforcing areas with bullet holes was necessary was erroneous. In reality, other areas should have been reinforced since planes with bullet holes were still able to return, while others couldn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Explain the omitted variable bias with an example from the course**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omitted variable bias occurs when an important explanatory variable is left out of the model, and this variable is correlated with the included explanatory variables. This can lead to biased estimates of the coefficients of the included variables. An example from the course could be estimating the effect of education on income, but failing to include ability as an explanatory variable. Since ability is likely correlated with both education and income, omitting it from the model would lead to biased estimates of the effect of education on income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Explain the problem of multicollinearity. Is it a problem in this dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of multicollinearity occurs when two or more explanatory variables in a regression model are highly correlated with each other. This can lead to unstable and imprecise estimates of the coefficients, as it becomes difficult to disentangle the individual effects of the correlated variables. Multicollinearity can also make it difficult to determine the statistical significance of the individual coefficients. An example from the course could be estimating the effect of both years of education and years of work experience on income, as these two variables are likely to be highly correlated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_df = np.linalg.det(df.T @ df)\n",
    "\n",
    "print(f\"Determinant of X'X is {det_df}\")\n",
    "\n",
    "if np.isclose(det_df, 0):\n",
    "    print(\"There is multicollinearity in the data.\")\n",
    "else:\n",
    "    print(\"There is no multicollinearity in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, this dataset contains some highly correlated coefficients, such as radial the and the proptax with a correlation of 0.91. Therefore, we can state that the multicollinearity is a problem in this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Create three categories of nox levels (low, medium, high), corresponding to the following percentiles: 0-25%, 26%-74%, 75%-100%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = df['nox'].quantile([0.25, 0.75])\n",
    "df['nox_level'] = pd.cut(df['nox'], bins=[df['nox'].min(), low, high, df['nox'].max()], labels=['low', 'medium', 'high'], include_lowest=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7. Compute for each category of nox level the average median price and comment on your results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "grouped = df.groupby('nox_level')\n",
    "average_prices = grouped['price'].mean()\n",
    "\n",
    "average_pricestable = pd.DataFrame({'NOx level': average_prices.index, 'Average Price': average_prices.values})\n",
    "average_pricestablec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8. Produce a scatter plot with the variable price on the y-axis and the variable nox on the x-axis. Is this a ceteris paribus effect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['nox'], df['price'])\n",
    "plt.xlabel('NOX')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Scatter plot of Price vs NOX')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing the graph, it is possible to observe that the average prices increase as NOx levels rise. However, it cannot be asserted that this is a ceteris paribus effect because we do not know if the other variables are held constant, and a scatter plot alone is not sufficient. Moreover, it is possible to see that there are different prices for the same NOx level, indicating the likely presence of other variables influencing the price, which are therefore not constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9. Run a regression of price on a constant, crime, nox, rooms, proptax. Comment on the histogram of the residuals. Interpret all coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['crime', 'nox', 'rooms', 'proptax']]\n",
    "y_9 = df['price']\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y_9, X)\n",
    "results9 = model.fit()\n",
    "print(results9.summary())\n",
    "\n",
    "plt.hist(results9.resid, bins='auto')\n",
    "plt.title('Histogram of residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS regression results show that all the variables (crime, nox, rooms, proptax) are statistically significant to the price variation as their p-values are less than 0.05.\n",
    "\n",
    "The coefficients indicate that the price is negatively correlated with crime, nox, and proptax, and positively correlated with rooms.\n",
    "\n",
    "The histogram of residuals follows a normal distribution. It suggests that the linear regression model might fit for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **10. Run a regression of lprice on a *constant, crime, nox, rooms, proptax*. Interpret all coefficients.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['crime', 'nox', 'rooms', 'proptax']]\n",
    "y_10 = df['lprice']\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y_10, X)\n",
    "results10 = model.fit()\n",
    "print(results10.summary())\n",
    "\n",
    "plt.hist(results10.resid, bins='auto')\n",
    "plt.title('Histogram of residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS regression results show that all the variables (crime, nox, rooms, proptax) are statistically significant to lprice variation as their p-values are less than 0.05.\n",
    "\n",
    "The coefficients indicate that the price is negatively correlated with crime, nox, and proptax, and positively correlated with rooms.\n",
    "\n",
    "The histogram of residuals follows a normal distribution. It suggests that the linear regression model might fit for this data.\n",
    "\n",
    "Comparing with the results of the previous model, the coefficients are smaller in this model. And the R-squared is higher in this model, which means that this model fits better than the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **11. Run a regression of lprice on a *constant, crime, lnox, rooms, lproptax*. Interpret all coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['crime', 'lnox', 'rooms', 'lproptax']]\n",
    "y = df['lprice']\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results11 = model.fit()\n",
    "print(results11.summary())\n",
    "\n",
    "plt.hist(results11.resid, bins='auto')\n",
    "plt.title('Histogram of residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS regression results show that all the variables (crime, lnox, rooms, lproptax) are statistically significant to lprice variation as their p-values are less than 0.05.\n",
    "\n",
    "The coefficients indicate that the price is negatively correlated with crime, lnox, and lproptax, and positively correlated with rooms.\n",
    "\n",
    "The histogram of residuals follows a normal distribution. It suggests that the linear regression model might fit for this data.\n",
    "\n",
    "The R-squared is a little bit higher than the previous model, which means that this model fits better than the previous one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **12. In the specification of question 9, test the hypothesis H0: βnox = 0 vs. H1: βnox ≠ 0 at the 1% level using the p-value of the test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "\n",
    "t_statistic = results10.params['crime'] / results10.bse['crime']\n",
    "\n",
    "k = len(results10.params) - 1\n",
    "ndf = len(y_10) - k - 1\n",
    "\n",
    "p_value = t.cdf(abs(t_statistic), df=ndf)\n",
    "\n",
    "\n",
    "print('T-statistic:', t_statistic)\n",
    "print('P-value:', p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"We reject the null hypothesis at the 1% level.\")\n",
    "else:\n",
    "    print(\"We do not reject the null hypothesis at the 1% level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **13. In the specification of question 9, test the hypothesis $H0: βcrime = βproptax at the 10%$ level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = results10.pvalues['nox']\n",
    "alpha = 0.01\n",
    "print('p_value = ', p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"We reject the null hypothesis at the 1% level.\")\n",
    "else:\n",
    "    print(\"We do not reject the null hypothesis at the 1% level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **14. In the specification of question 9, test the hypothesis $H_{0}: β_{nox} = 0, β_{proptax} = 0$ at the 10% level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:\n",
    "\n",
    "$$\n",
    "\\text{{lprice}} = \\beta_0 + \\beta_1 \\cdot \\text{{crime}} + \\beta_2 \\cdot \\text{{nox}} + \\beta_3 \\cdot \\text{{rooms}} + \\beta_4 \\cdot \\text{{proptax}} + u\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta = \\beta_1 - \\beta_4\n",
    "$$\n",
    "\n",
    "Hypotheses:\n",
    "\n",
    "$$\n",
    "H_0: \\theta = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_1: \\theta \\neq 0\n",
    "$$\n",
    "\n",
    "Expressing $\\beta_1$ in terms of $\\theta$ and $\\beta_4$:\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\theta + \\beta_4\n",
    "$$\n",
    "\n",
    "Substituting $\\beta_1$ back into the model:\n",
    "\n",
    "$$\n",
    "\\text{{lprice}} = \\beta_0 + (\\theta + \\beta_4) \\cdot \\text{{crime}} + \\beta_2 \\cdot \\text{{nox}} + \\beta_3 \\cdot \\text{{rooms}} + \\beta_4 \\cdot \\text{{proptax}} + u\n",
    "$$\n",
    "$$\n",
    "\\text{{lprice}} = \\beta_0 + \\theta \\cdot \\text{{crime}} + \\beta_2 \\cdot \\text{{nox}} + \\beta_3 \\cdot \\text{{rooms}} + \\beta_4 \\cdot (\\text{{crime}} + \\text{{proptax}}) + u\n",
    "$$\n",
    "\n",
    "Creating a new variable:\n",
    "\n",
    "$$\n",
    "\\text{{crime\\_tax}} = \\text{{crime}} + \\text{{proptax}}\n",
    "$$\n",
    "\n",
    "Perform OLS regression:\n",
    "\n",
    "$$\n",
    "\\text{{model}}: \\quad \\text{{lprice}} = \\beta_0 + \\theta \\cdot \\text{{crime}} + \\beta_2 \\cdot \\text{{nox}} + \\beta_3 \\cdot \\text{{rooms}} + \\beta_4 \\cdot (\\text{{crime}} + \\text{{proptax}}) + u\n",
    "$$\n",
    "\n",
    "Hypothesis test:\n",
    "\n",
    "$$\n",
    "H_0: b_j = a_j  \\quad \\Rightarrow \\quad \\theta = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "t = \\frac{{b_j - a_j}}{{\\text{{se}}(b_j)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "t = \\frac{{\\theta}}{{\\text{{se}}(\\theta)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['crime', 'nox', 'rooms']].copy()\n",
    "X['crime_tax'] = X['crime'] + df['proptax']\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = df['lprice']\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results14 = model.fit()\n",
    "\n",
    "# Hypothesis test:\n",
    "# t = theta / se(theta)\n",
    "t_stat = results14.params['crime'] / results14.bse['crime']\n",
    "\n",
    "alpha = 0.1\n",
    "k = len(results14.params) - 1 \n",
    "ndf = len(y) - k - 1 # number of degrees of freedom\n",
    "p_value = 2 * t.sf(abs(t_stat), df=ndf)\n",
    "\n",
    "print('p_value:', p_value)\n",
    "print('t-stat:', t_stat)\n",
    "\n",
    "if p_value < alpha:\n",
    "  print(\"We reject the null hypothesis at the\", alpha*100, \"% level.\")\n",
    "else:\n",
    "  print(\"We do not reject the null hypothesis at the\", alpha*100, \"% level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **15. In the specification of question 9, test the hypothesis $H0: βnox = -500, βproptax = -100$ at the 10% level using the p-value of the test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unrestricted = sm.add_constant(df[['crime', 'nox', 'rooms', 'proptax']])\n",
    "x_restricted = sm.add_constant(df[['crime', 'rooms']])\n",
    "\n",
    "y = df['lprice']\n",
    "\n",
    "model_unrestricted = sm.OLS(y, x_unrestricted)\n",
    "model_restricted = sm.OLS(y, x_restricted)\n",
    "\n",
    "results_unrestricted = model_unrestricted.fit()\n",
    "results_restricted = model_restricted.fit()\n",
    "\n",
    "# Hypothesis test:\n",
    "SSR_unrestricted = results_unrestricted.ssr\n",
    "SSR_restricted = results_restricted.ssr\n",
    "\n",
    "k_unrestricted = x_unrestricted.shape[1] - 1\n",
    "k_restricted = x_restricted.shape[1] - 1\n",
    "\n",
    "q = k_unrestricted - k_restricted  # numerator degrees of freedom\n",
    "n = len(y)\n",
    "ddf = n - k_unrestricted - 1  # denominator degrees of freedom\n",
    "\n",
    "F_statistic = ((SSR_restricted - SSR_unrestricted) / q) / \\\n",
    "    (SSR_unrestricted / ddf)\n",
    "\n",
    "\n",
    "alpha = 0.10  # 10% significance level\n",
    "p_value = 2 * f.sf(F_statistic, q, ddf)\n",
    "\n",
    "print(f'F Statistic: {F_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "if p_value < alpha:\n",
    "    print('Reject the null hypothesis')\n",
    "else:\n",
    "    print('Fail to reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **16. In the specification of question 9, test the hypothesis that all coefficients are the same for observations with low levels of nox vs. medium and high levels of *nox*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unrestricted model:\n",
    "\n",
    "$$\n",
    "\\text{{lprice}} = \\beta_0 + \\beta_1 \\cdot \\text{{crime}} + \\beta_2 \\cdot \\text{{nox}} + \\beta_3 \\cdot \\text{{rooms}} + \\beta_4 \\cdot \\text{{proptax}} + u\n",
    "$$\n",
    "\n",
    "Restricted model:\n",
    "\n",
    "$$\n",
    "\\text{{lprice}} - \\beta_2 \\cdot \\text{{nox}} - \\beta_4 \\cdot \\text{{proptax}} = \\beta_0 + \\beta_1 \\cdot \\text{{crime}} + \\beta_3 \\cdot \\text{{rooms}}\n",
    "$$\n",
    "\n",
    "Given values:\n",
    "\n",
    "$$\n",
    "\\beta_2 = -500, \\quad \\beta_4 = -100\n",
    "$$\n",
    "\n",
    "Substituting the values into the restricted model:\n",
    "\n",
    "$$\n",
    "\\text{{lprice}} + 500 \\cdot \\text{{nox}} + 100 \\cdot \\text{{proptax}} = \\beta_0 + \\beta_1 \\cdot \\text{{crime}} + \\beta_3 \\cdot \\text{{rooms}}\n",
    "$$\n",
    "\n",
    "This represents the restricted model with the specified values for $\\beta_2$ and $\\beta_4$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unrestricted = sm.add_constant(df[['crime', 'nox', 'rooms', 'proptax']])\n",
    "x_restricted = sm.add_constant(df[['crime', 'rooms']])\n",
    "\n",
    "y_unrestricted = df['lprice']\n",
    "y_restricted = df['lprice'] + 500 * df['nox'] + 100 * df['proptax']\n",
    "\n",
    "model_unrestricted = sm.OLS(y_unrestricted, x_unrestricted)\n",
    "model_restricted = sm.OLS(y_restricted, x_restricted)\n",
    "\n",
    "results_unrestricted = model_unrestricted.fit()\n",
    "results_restricted = model_restricted.fit()\n",
    "\n",
    "# Hypothesis test:\n",
    "SSR_unrestricted = results_unrestricted.ssr\n",
    "SSR_restricted = results_restricted.ssr\n",
    "\n",
    "k_unrestricted = x_unrestricted.shape[1] - 1\n",
    "k_restricted = x_restricted.shape[1] - 1\n",
    "\n",
    "q = k_unrestricted - k_restricted  # numerator degrees of freedom\n",
    "n = len(y)\n",
    "ddf = n - k_unrestricted - 1  # denominator degrees of freedom\n",
    "\n",
    "F_statistic = ((SSR_restricted - SSR_unrestricted) / q) / \\\n",
    "    (SSR_unrestricted / ddf)\n",
    "\n",
    "\n",
    "p_value = 2 * f.sf(F_statistic, q, ddf)\n",
    "\n",
    "alpha = 0.10  # 10% significance level\n",
    "\n",
    "print(f'P-Value: {p_value}')\n",
    "print(f'F-Statistic: {F_statistic}')\n",
    "\n",
    "if p_value < alpha:\n",
    "    print('Reject the null hypothesis')\n",
    "else:\n",
    "    print('Fail to reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **17. Repeat the test of question 16 but now assuming that only the coefficients of nox and proptax can change between the two groups of observations. State and test H_{0}.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{{model}}: \\text{{lprice}} = \\beta_0 + \\beta_1 \\cdot \\text{{crime}} + \\beta_2 \\cdot \\text{{nox}} + \\beta_3 \\cdot \\text{{rooms}} + \\beta_4 \\cdot \\text{{proptax}} + u\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta = \\beta_2 + \\beta_4\n",
    "$$\n",
    "\n",
    "Hypotheses:\n",
    "- $H_0: \\theta = -1000$\n",
    "- $H_1: \\theta \\neq -1000$\n",
    "\n",
    "Expressing $\\beta_2$ in terms of $\\theta$ and $\\beta_4$:\n",
    "\n",
    "$$\n",
    "\\beta_2 = \\theta - \\beta_4\n",
    "$$\n",
    "\n",
    "Substituting $\\beta_2$ back into the model:\n",
    "\n",
    "$$\n",
    "\\text{{lprice}} = \\beta_0 + \\beta_1 \\cdot \\text{{crime}} + (\\theta - \\beta_4) \\cdot \\text{{nox}} + \\beta_3 \\cdot \\text{{rooms}} + \\beta_4 \\cdot \\text{{proptax}} + u\n",
    "$$\n",
    "$$\n",
    "\\text{{lprice}} = \\beta_0 + \\beta_1 \\cdot \\text{{crime}} + \\theta \\cdot \\text{{nox}} + \\beta_3 \\cdot \\text{{rooms}} + \\beta_4 \\cdot \n",
    "(\\text{{proptax}} - \\text{{nox}}) + u\n",
    "$$\n",
    "Creating a new variable:\n",
    "\n",
    "$$\n",
    "\\text{{proptax\\_nox}} = \\text{{proptax}} - \\text{{nox}}\n",
    "$$\n",
    "\n",
    "Perform OLS regression:\n",
    "\n",
    "$$\n",
    "\\text{{model}}: \\quad \\text{{lprice}} = \\beta_0 + \\beta_1 \\cdot \\text{{crime}} + \\theta \\cdot \\text{{nox}} + \\beta_3 \\cdot \\text{{rooms}} + \\beta_4 \\cdot \\text{{proptax\\_nox}} + u\n",
    "$$\n",
    "\n",
    "Hypothesis test:\n",
    "\n",
    "$$\n",
    "H_0: b_j = a_j  \\quad \\Rightarrow \\quad \\theta = -1000\n",
    "$$\n",
    "\n",
    "$$\n",
    "t = \\frac{{b_j - a_j}}{{\\text{{se}}(b_j)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "t = \\frac{{\\theta + 1000}}{{\\text{{se}}(\\theta)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['crime', 'nox', 'rooms']].copy()\n",
    "X['proptax_nox'] = df['proptax'] - df['nox']\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = df['lprice']\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results14 = model.fit()\n",
    "\n",
    "beta_1 = results14.params['nox']\n",
    "t_stat = (beta_1 + 1000) / results14.bse['nox']\n",
    "\n",
    "alpha = 0.1\n",
    "k = len(results14.params) - 1\n",
    "ndf = len(y) - k - 1 # number of degrees of freedom. \n",
    "\n",
    "p_value = 2 * t.sf(abs(t_stat), df=ndf)\n",
    "\n",
    "print('P-value:', p_value)\n",
    "print('t-stat:', t_stat)\n",
    "\n",
    "if p_value < alpha:\n",
    "  print(\"We reject the null hypothesis at the\", alpha*100, \"% level.\")\n",
    "else:\n",
    "  print(\"We do not reject the null hypothesis at the\", alpha*100, \"% level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Heteroskedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **18. Explain the problem of heteroskedasticity with an example of the course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step: estimate the coefficient for low levels of nox:\n",
    "\n",
    "$$\n",
    "\\text{{model}}: \\quad \\text{{lprice}} = \\beta_0 + \\beta_1 \\cdot \\text{{crime}} + \\beta_2 \\cdot \\text{{nox}} + \\beta_3 \\cdot \\text{{rooms}} + \\beta_4 \\cdot \\text{{proptax}} + u\n",
    "$$\n",
    "\n",
    "Now we know the coefficients, and we can make the hypothesis test for medium and high levels of nox:\n",
    "\n",
    "- $H_0$: $b_{i_{\\text{{low}}}} = b_{i_{\\text{{high\\_medium}}}} \\quad \\forall \\; i$\n",
    "- $H_1$: $b_{i_{\\text{{low}}}} \\neq b_{i_{\\text{{high\\_medium}}}} \\quad \\exists \\; i$\n",
    "\n",
    "$$\n",
    "\\text{{Restricted model}}: \\quad \\text{{lprice}} - \\beta_1 \\cdot \\text{{crime}} - \\beta_2 \\cdot \\text{{nox}} - \\beta_3 \\cdot \\text{{rooms}} - \\beta_4 \\cdot \\text{{proptax}} = \\beta_0 + u\n",
    "$$\n",
    "\n",
    "where $\\beta_n \\forall\\; n \\in \\{1,2,3,4\\}$ are the coefficients estimated in the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: estimate the coefficient for low levels of nox\n",
    "x_unrestricted = df[df['nox_level'] == 'low'][[\n",
    "    'crime', 'nox', 'rooms', 'proptax']]\n",
    "x_unrestricted = sm.add_constant(x_unrestricted)\n",
    "\n",
    "y_unrestricted = df[df['nox_level'] == 'low']['lprice']\n",
    "\n",
    "model = sm.OLS(y_unrestricted, x_unrestricted)\n",
    "results_unrestricted = model.fit()\n",
    "\n",
    "# Restricted model:\n",
    "\n",
    "x_medium_high = df[df['nox_level'] != 'low'][[\n",
    "    'crime', 'nox', 'rooms', 'proptax']]\n",
    "y_medium_high = df[df['nox_level'] != 'low']['lprice']\n",
    "\n",
    "x_restricted = sm.add_constant(x_medium_high)[\n",
    "    ['const']]  # Only the constant goes here\n",
    "y_restricted = y_medium_high - results_unrestricted.params['crime'] * x_medium_high['crime'] \\\n",
    "    - results_unrestricted.params['nox'] * x_medium_high['nox']\\\n",
    "    - results_unrestricted.params['rooms'] * x_medium_high['rooms'] \\\n",
    "    - results_unrestricted.params['proptax'] * x_medium_high['proptax']\n",
    "\n",
    "model_restricted = sm.OLS(y_restricted, x_restricted)\n",
    "results_restricted = model_restricted.fit()\n",
    "\n",
    "# Hypothesis test:\n",
    "SSR_unrestricted = results_unrestricted.ssr\n",
    "SSR_restricted = results_restricted.ssr\n",
    "\n",
    "k_unrestricted = x_unrestricted.shape[1] - 1\n",
    "k_restricted = x_restricted.shape[1] - 1\n",
    "\n",
    "q = k_unrestricted - k_restricted  # numerator degrees of freedom\n",
    "\n",
    "n = len(y)\n",
    "ddf = n - k_unrestricted - 1  # denominator degrees of freedom\n",
    "\n",
    "F_statistic = ((SSR_restricted - SSR_unrestricted) / q) / \\\n",
    "    (SSR_unrestricted / ddf)\n",
    "\n",
    "\n",
    "alpha = 0.10  # 10% significance level\n",
    "p_value = f.sf(F_statistic, q, ddf)\n",
    "\n",
    "print(f'P-Value: {p_value}')\n",
    "print(f'F-Statistic: {F_statistic}')\n",
    "\n",
    "if p_value < alpha:\n",
    "    print('Reject the null hypothesis')\n",
    "else:\n",
    "    print('Fail to reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **19. In the specification of question 9, test the hypothesis of no heteroskedasticity of linear form, i.e. in the regression of u2 on constant, crime, nox, rooms, proptax, test $H_{0}: \\delta_{crime}, \\delta_{nox}, \\delta_{room}, \\delta_{proptax} = 0$, where the coefficients $\\delta_{k}$ (k = crime, nox, rooms, proptax) are associated with the corresponding explanatory variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the coefficients, we can conduct a hypothesis test for medium and high levels of NOx. The hypotheses are as follows:\n",
    "\n",
    "- Null Hypothesis ($H_0$): $b_{i_{\\text{low}}} = b_{i_{\\text{high\\_medium}}} \\quad \\forall \\; i \\in \\{2,4\\}$\n",
    "- Alternative Hypothesis ($H_1$): $b_{i_{\\text{low}}} \\neq b_{i_{\\text{high\\_medium}}} \\quad \\text{for some } i \\in \\{2,4\\}$\n",
    "\n",
    "We have two models:\n",
    "\n",
    "1. **Unrestricted model:**\n",
    "   - $ \\text{lprice} = \\beta_0 + \\beta_1 \\cdot \\text{crime} + \\beta_2 \\cdot \\text{nox} + \\beta_3 \\cdot \\text{rooms} + \\beta_4 \\cdot \\text{proptax} + u $\n",
    "\n",
    "2. **Restricted model:**\n",
    "   - $ \\text{lprice} - \\beta_2 \\cdot \\text{nox} - \\beta_4 \\cdot \\text{proptax} = \\beta_0 + \\beta_1 \\cdot \\text{crime} + \\beta_3 \\cdot \\text{rooms} + u $\n",
    "   - where $ \\beta_2 $ and $ \\beta_4 $ are the results of the unrestricted model\n",
    "\n",
    "considering $\\alpha = 0.1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: estimate the coefficient for low levels of nox\n",
    "x_unrestricted = df[df['nox_level'] == 'low'][[\n",
    "    'crime', 'nox', 'rooms', 'proptax']]\n",
    "x_unrestricted = sm.add_constant(x_unrestricted)\n",
    "\n",
    "y_unrestricted = df[df['nox_level'] == 'low']['lprice']\n",
    "\n",
    "model = sm.OLS(y_unrestricted, x_unrestricted)\n",
    "results_unrestricted = model.fit()\n",
    "\n",
    "# Second step: estimate the restricted model for medium and high levels of nox\n",
    "x_medium_high = df[df['nox_level'] != 'low'][[\n",
    "    'crime', 'nox', 'rooms', 'proptax']]\n",
    "x_restricted = x_medium_high[['crime', 'rooms']]\n",
    "x_restricted = sm.add_constant(x_restricted)\n",
    "\n",
    "y_medium_high = df[df['nox_level'] != 'low']['lprice']\n",
    "y_restricted = y_medium_high - results_unrestricted.params['nox'] * x_medium_high['nox']\\\n",
    "    - results_unrestricted.params['proptax'] * x_medium_high['proptax']\n",
    "\n",
    "model_restricted = sm.OLS(y_restricted, x_restricted)\n",
    "results_restricted = model_restricted.fit()\n",
    "\n",
    "# Hypothesis test:\n",
    "SSR_unrestricted = results_unrestricted.ssr\n",
    "SSR_restricted = results_restricted.ssr\n",
    "\n",
    "k_unrestricted = x_unrestricted.shape[1] - 1\n",
    "k_restricted = x_restricted.shape[1] - 1\n",
    "\n",
    "q = k_unrestricted - k_restricted  # numerator degrees of freedom\n",
    "\n",
    "n = len(y)\n",
    "ddf = n - k_unrestricted - 1  # denominator degrees of freedom\n",
    "\n",
    "F_statistic = ((SSR_restricted - SSR_unrestricted) / q) / \\\n",
    "    (SSR_unrestricted / ddf)\n",
    "\n",
    "\n",
    "alpha = 0.10  # 10% significance level\n",
    "p_value = f.sf(F_statistic, q, ddf)\n",
    "\n",
    "print(f'P-Value: {p_value}')\n",
    "print(f'F-Statistic: {F_statistic}')\n",
    "\n",
    "if p_value < alpha:\n",
    "    print('Reject the null hypothesis')\n",
    "else:\n",
    "    print('Fail to reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **20. In the specification of question 10, test the hypothesis of no heteroskedasticity of linear form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['crime', 'nox', 'rooms', 'proptax']]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "u = results10.resid\n",
    "u2 = u**2\n",
    "y = u2\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "f_statistic = results.fvalue\n",
    "\n",
    "alpha = 0.1\n",
    "k = len(results.params) - 1\n",
    "n = len(y)\n",
    "ddf = n - k - 1\n",
    "\n",
    "p_value = f.sf(f_statistic, dfn=k, dfd=ddf)\n",
    "\n",
    "print('P-value = ', p_value)\n",
    "print('F-statistic = ', f_statistic)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"We reject the null hypothesis at the 1% level.\")\n",
    "else:\n",
    "    print(\"We do not reject the null hypothesis at the 1% level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **21. In the specification of question 11, test the hypothesis of no heteroskedasticity of linear form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['crime', 'lnox', 'rooms', 'lproptax']]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "u = results11.resid\n",
    "u2 = u**2\n",
    "y = u2\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "f_statistic = results.fvalue\n",
    "\n",
    "alpha = 0.1\n",
    "k = len(results.params) - 1\n",
    "n = len(y)\n",
    "ddf = n - k - 1\n",
    "\n",
    "p_value = f.sf(f_statistic, dfn=k, dfd=ddf)\n",
    "\n",
    "print('P-value:', p_value)\n",
    "print('F-statistic = ', f_statistic)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"We reject the null hypothesis at the 1% level.\")\n",
    "else:\n",
    "    print(\"We do not reject the null hypothesis at the 1% level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **22. Comment on the differences between your results of questions 20,21, 22.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all three tests, the null hypothesis is rejected, indicating the presence of heteroskedasticity. However, an increase in the F-statistic is noticeable when comparing the tests. In the first test, the F-value is 6.799; in the second, F is 19.98; and in the third, F is 18.27. The increment in the F-value is noteworthy because as it increases, it moves further away from the critical value, which remains constant at 1.95 for all tests. Therefore, it can be concluded that as the F-value rises, we gain more certainty about the presence of heteroskedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_22 = df[['crime', 'nox', 'rooms', 'proptax']]\n",
    "x_22 = sm.add_constant(x_22)\n",
    "\n",
    "u = results9.resid\n",
    "u2 = u**2\n",
    "y_22 = u2\n",
    "model = sm.OLS(y_22, x_22)\n",
    "results22 = model.fit()\n",
    "\n",
    "f_statistic = results22.fvalue\n",
    "\n",
    "alpha = 0.1\n",
    "k = len(results22.params) - 1\n",
    "n = len(y_22)\n",
    "ddf = n - k - 1\n",
    "\n",
    "p_value = f.sf(f_statistic, dfn=k, dfd=ddf)\n",
    "\n",
    "print('P-value = ', p_value)\n",
    "print('F-statistic = ', f_statistic)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"We reject the null hypothesis at the 1% level.\")\n",
    "else:\n",
    "    print(\"We do not reject the null hypothesis at the 1% level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **23. Using the specification of question 9, identify the most significant variable causing heteroskedasticity using the student statistics and run a WLS regression with the identified variable as weight. Compare the standards errors with those of question 9. Comment on your results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Time Series Data\n",
    "Using the threecenturies_v2.3 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **24. Define strict and weak stationarity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **25. Explain ergodicity and state the ergodic theorem. Illustrate with an example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **26. Why do we need both stationarity and ergodicity?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **27. Explain “spurious regression”.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **28. Make all time series stationary by computing the difference between the original variable and a moving average of order 2x10. Give the formula for the exact weights.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **29. Using the original dataset, test the unit root hypothesis for all variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **30. Transform all variables so that they are stationary using either your answers to questions 28 or to question 29.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **31. Explain the difference between ACF and PACF.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **32. Plot and comment on the ACF and PACF of all variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **33. Explain the principle of parsimony and its relationship with Ockham’s razor using the theory of information criterion.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **34. Explain the problem of auto-correction of the errors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **35. Using only stationary variables, run a regression of GDP on constant, unemployment and inflation and test the hypothesis of no-autocorrelation of errors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **36. Regardless of your answer to question 35, correct auto-correlation with GLS. Test again for the presence of auto-correlation. Comment on your results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **37. For all variables, construct their lag 1 and lag 2 variables..**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **38. Run a regression of GDP on constant, lag 1 unemployment, lag 2 unemployment, lag 1 inflation, lag 2 inflation. What is the number of observations and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **39. State and test the no-Granger causality hypothesis of unemployment on GDP at the 1% level.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **40. Divide the sample in two groups: 1900-1960 and 1961-2000. Test the stability of coefficients between the two periods.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **41. Test the structural breakpoint using a trim ratio of 30% at the 1% level.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **42. Divide the sample into 3 periods of equal length. Test that the coefficients of the second and the third periods are equal. Formulate the null hypothesis and interpret your results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
